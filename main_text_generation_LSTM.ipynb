{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_text_generation_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMEKr3s8yqtdOIMN8MB2ADA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhijitsahoo0790/text_generation_using_LSTM/blob/master/main_text_generation_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ugM54h3E41H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Mount google drive to google Colab environment\n",
        "from os.path import join\n",
        "from google.colab import drive\n",
        "\n",
        "ROOT = \"/content/drive\"\n",
        "drive.mount(ROOT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb6LXyNYFJN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "For creating a new project in GitHub, it will throw error if it is executed after the project dir is created\n",
        "\"\"\"\n",
        "ROOT = \"/content/drive\"\n",
        "PROJ = \"My Drive/Colab Notebooks/text_generation_using_LSTM\" # This is a custom path.\n",
        "PROJECT_PATH = join(ROOT, PROJ)\n",
        "!mkdir \"{PROJECT_PATH}\"\n",
        "!git clone https://github.com/abhijitsahoo0790/text_generation_using_LSTM.git \"{PROJECT_PATH}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r28Ok3GGoJKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ROOT = \"/content/drive\"\n",
        "PROJ = \"My Drive/Colab Notebooks/text_generation_using_LSTM\" # This is a custom path.\n",
        "PROJECT_PATH = join(ROOT, PROJ)\n",
        "%cd \"{PROJECT_PATH}\"\n",
        "!git pull origin master"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N40o2oB7GVw2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b4cd34f-0366-4aff-fb48-8b05149c9cb1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from random import randint\n",
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "import copy\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import traceback\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s %(levelname)-8s %(message)s', \n",
        "                    filename='log.txt', filemode='w', level=logging.DEBUG, \n",
        "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
        "START_DELIMITER = \"ssttaarrt\"\n",
        "END_DELIMITER = \"eenndd\"\n",
        "WINDOW_LENGTH = 20"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LqDFQH6r2zq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fetch_the_corpora_using_NLTK():\n",
        "    \"\"\"\n",
        "    Return the unified corpora from NLTK corpora.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    text : str\n",
        "        Text data of the corpora.\n",
        "    \"\"\"\n",
        "    corpous_name = \"brown\"\n",
        "    status = nltk.download(corpous_name)\n",
        "    if (status):\n",
        "        logging.info(\"Downloaded Brown corpus\")\n",
        "        mdetok = TreebankWordDetokenizer()\n",
        "        brown_natural = [mdetok.detokenize(' '.join(sent).replace('``', '\"').replace(\"''\", '\"').replace('`', \"'\").split())  for sent in brown.sents()]\n",
        "        logging.info(\"Processed Brown corpus as text\")\n",
        "    else:\n",
        "        logging.error(\"Couldn't download the \"+ corpous_name+\" corpus\")\n",
        "        \n",
        "    return brown_natural\n",
        "\n",
        "def enumerate_text_using_word_enum_dict(unified_corpora, word_enum_dict):\n",
        "    \"\"\"\n",
        "    Enumerate the complete text in corpous using word_enum_dict\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    unified_corpora : TYPE\n",
        "        DESCRIPTION.\n",
        "    word_enum_dict : TYPE\n",
        "        DESCRIPTION.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    complete_text_enumerated : TYPE\n",
        "        DESCRIPTION.\n",
        "\n",
        "    \"\"\"\n",
        "    complete_text = \" \".join([START_DELIMITER+\" \"+item+\" \"+END_DELIMITER for item in unified_corpora])\n",
        "    complete_text_processed = re.sub(' +', ' ', re.sub('[^A-Za-z ]+', ' ',complete_text.lower())).strip()\n",
        "    complete_text_enumerated =  [word_enum_dict[item] for item in complete_text_processed.split(\" \") if item in word_enum_dict]   \n",
        "    return complete_text_enumerated\n",
        "\n",
        "def enumerate_unique_words(text_corpus):\n",
        "    \"\"\"\n",
        "    Enumerate unique words and return its dictionary and reversed-dictionary\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    unified_corpora : list of str\n",
        "        The text corpora as a list of words \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    word_enum_dict\n",
        "        word as key and its integer enumeration as the value.\n",
        "    reversed_word_enum_dict\n",
        "        word as value and its integer enumeration as the key.\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    Join all sentences, remove special characters except Space, split all \n",
        "    words, take set for unique words, convert it to list, remove None values using filter\n",
        "    \"\"\"\n",
        "    unique_words = list(filter(None, list(set(re.sub('[^A-Za-z ]+', ' ', (text_corpus)).split(\" \")))))\n",
        "    unique_words = unique_words + [START_DELIMITER, END_DELIMITER]\n",
        "    #enumerate unique words\n",
        "    word_enum_dict = {v:k for k,v in enumerate(unique_words)}\n",
        "    reversed_word_enum_dict = {k:v for k,v in enumerate(unique_words)}\n",
        "    return [word_enum_dict, reversed_word_enum_dict]\n",
        "\n",
        "def generate_sequence_data_for_LSTM(complete_text_enumerated):\n",
        "    \"\"\"\n",
        "    Generate pattern sequences of length as specified by WINDOW_LENGTH and \n",
        "    also generate target of the patterns generated.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    complete_text_enumerated : list of int\n",
        "        Enumerated text sequence.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X\n",
        "        Reshaped pattern sequences for LSTM input .\n",
        "    y\n",
        "        Target for each generated patterns.\n",
        "    \"\"\"\n",
        "    pattern_sequence = []\n",
        "    pattern_targets = []\n",
        "    for i in range(0, len(complete_text_enumerated)-WINDOW_LENGTH):\n",
        "        temp_pattern = complete_text_enumerated[i:i+WINDOW_LENGTH]\n",
        "        temp_pattern_target = complete_text_enumerated[i+WINDOW_LENGTH]\n",
        "        pattern_sequence.append(temp_pattern)\n",
        "        pattern_targets.append(temp_pattern_target)\n",
        "    num_patterns = len(pattern_sequence)\n",
        "    X = np.reshape(pattern_sequence, (num_patterns, WINDOW_LENGTH))\n",
        "    y = np_utils.to_categorical(pattern_targets)\n",
        "    return [X, y]\n",
        "\n",
        "\n",
        "def remove_special_chars(text):\n",
        "    \"\"\"\n",
        "    Remove all special characters except space and remove extra spaces. \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        Any text\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    text : str\n",
        "        Processed text\n",
        "    \"\"\"\n",
        "    \n",
        "    text = re.sub(' +', ' ', re.sub('[^A-Za-z ]+', ' ', text).strip())\n",
        "    return text\n",
        "\n",
        "def fetch_corpous_from_file(filepath):    \n",
        "    \"\"\"\n",
        "    Read a corpus and do basic processing.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filepath : str\n",
        "        Path of the text corpus\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    corpus_list_sent_processed : list of str\n",
        "        processed corpus in form of list of str.\n",
        "\n",
        "    \"\"\"\n",
        "    f = open(filepath, 'r')\n",
        "    corpus_text = f.read()    \n",
        "    corpus_sentence_list = corpus_text.lower().split('.')\n",
        "    corpus_list_sent_processed = [remove_special_chars(item) for item in corpus_sentence_list if len(item)>1] \n",
        "    return corpus_list_sent_processed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgoQIOBgGa6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a99d487b-dc54-4431-8b86-df0c7aebed33"
      },
      "source": [
        "PROJ_PATH = \"/content/drive/My Drive/Colab Notebooks/text_generation_using_LSTM/\"\n",
        "FILE_PATH = \"data/republic.txt\"\n",
        "FULL_PATH_FILE = join(PROJ_PATH, FILE_PATH)\n",
        "%cd \"{PROJ_PATH}\"\n",
        "if __name__ == \"__main__\":\n",
        "    logging.info(\"Fetching text corpus...\")\n",
        "    # unified_corpora = fetch_the_corpora_using_NLTK() \n",
        "    unified_corpora = fetch_corpous_from_file(FULL_PATH_FILE)  \n",
        "    logging.info(\"Fetched text corpus\")\n",
        "    \n",
        "    # Enumerate unique words\n",
        "    [word_enum_dict, reversed_word_enum_dict] = enumerate_unique_words(\" \".join(unified_corpora))\n",
        "    # Enumerate text using word_enum_dict\n",
        "    complete_text_enumerated = enumerate_text_using_word_enum_dict(unified_corpora, word_enum_dict)\n",
        "    # generate sequence data for training LSTM\n",
        "    [X, y] = generate_sequence_data_for_LSTM(complete_text_enumerated)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/text_generation_using_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLDv2rzrLLjk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "59607977-7e33-4e02-f1ea-e65f7ddcdac6"
      },
      "source": [
        "vocab_size = len(word_enum_dict) + 1\n",
        "seq_length = X.shape[1]\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size-1, activation='softmax'))\n",
        "print(model.summary())\n",
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "model.fit(X, y, batch_size=128, epochs=200)\n",
        "# save the model to file\n",
        "model.save('model_batchsize128.h5')\n",
        "\n",
        "# define the checkpoint\n",
        "#filepath=\"results/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "#checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "#callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 20, 100)           730000    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 20, 100)           80400     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 7299)              737199    \n",
            "=================================================================\n",
            "Total params: 1,638,099\n",
            "Trainable params: 1,638,099\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "126433/126433 [==============================] - 117s 929us/step - loss: 5.9637 - accuracy: 0.0891\n",
            "Epoch 2/200\n",
            "126433/126433 [==============================] - 117s 925us/step - loss: 5.4242 - accuracy: 0.1368\n",
            "Epoch 3/200\n",
            "126433/126433 [==============================] - 117s 924us/step - loss: 5.1583 - accuracy: 0.1657\n",
            "Epoch 4/200\n",
            "126433/126433 [==============================] - 118s 932us/step - loss: 4.9976 - accuracy: 0.1801\n",
            "Epoch 5/200\n",
            "126433/126433 [==============================] - 117s 929us/step - loss: 4.8743 - accuracy: 0.1900\n",
            "Epoch 6/200\n",
            "126433/126433 [==============================] - 117s 922us/step - loss: 4.7738 - accuracy: 0.1969\n",
            "Epoch 7/200\n",
            "126433/126433 [==============================] - 117s 926us/step - loss: 4.6846 - accuracy: 0.2015\n",
            "Epoch 8/200\n",
            "126433/126433 [==============================] - 117s 928us/step - loss: 4.5997 - accuracy: 0.2066\n",
            "Epoch 9/200\n",
            "126433/126433 [==============================] - 118s 930us/step - loss: 4.5168 - accuracy: 0.2114\n",
            "Epoch 10/200\n",
            "126433/126433 [==============================] - 117s 926us/step - loss: 4.4410 - accuracy: 0.2157\n",
            "Epoch 11/200\n",
            "126433/126433 [==============================] - 117s 928us/step - loss: 4.3666 - accuracy: 0.2206\n",
            "Epoch 12/200\n",
            "126433/126433 [==============================] - 118s 929us/step - loss: 4.2962 - accuracy: 0.2240\n",
            "Epoch 13/200\n",
            "126433/126433 [==============================] - 117s 927us/step - loss: 4.2300 - accuracy: 0.2273\n",
            "Epoch 14/200\n",
            "126433/126433 [==============================] - 117s 926us/step - loss: 4.1685 - accuracy: 0.2305\n",
            "Epoch 15/200\n",
            "126433/126433 [==============================] - 117s 926us/step - loss: 4.1085 - accuracy: 0.2336\n",
            "Epoch 16/200\n",
            "126433/126433 [==============================] - 116s 920us/step - loss: 4.0512 - accuracy: 0.2372\n",
            "Epoch 17/200\n",
            "126433/126433 [==============================] - 117s 922us/step - loss: 3.9978 - accuracy: 0.2403\n",
            "Epoch 18/200\n",
            "126433/126433 [==============================] - 116s 919us/step - loss: 3.9472 - accuracy: 0.2438\n",
            "Epoch 19/200\n",
            "126433/126433 [==============================] - 117s 922us/step - loss: 3.8976 - accuracy: 0.2476\n",
            "Epoch 20/200\n",
            "126433/126433 [==============================] - 117s 926us/step - loss: 3.8532 - accuracy: 0.2518\n",
            "Epoch 21/200\n",
            "126433/126433 [==============================] - 117s 922us/step - loss: 3.8109 - accuracy: 0.2552\n",
            "Epoch 22/200\n",
            "126433/126433 [==============================] - 116s 919us/step - loss: 3.7713 - accuracy: 0.2587\n",
            "Epoch 23/200\n",
            "126433/126433 [==============================] - 115s 913us/step - loss: 3.7340 - accuracy: 0.2626\n",
            "Epoch 24/200\n",
            "126433/126433 [==============================] - 116s 917us/step - loss: 3.6973 - accuracy: 0.2663\n",
            "Epoch 25/200\n",
            "126433/126433 [==============================] - 116s 919us/step - loss: 3.6638 - accuracy: 0.2696\n",
            "Epoch 26/200\n",
            "126433/126433 [==============================] - 116s 914us/step - loss: 3.6293 - accuracy: 0.2728\n",
            "Epoch 27/200\n",
            "126433/126433 [==============================] - 116s 918us/step - loss: 3.5990 - accuracy: 0.2773\n",
            "Epoch 28/200\n",
            "126433/126433 [==============================] - 117s 926us/step - loss: 3.5675 - accuracy: 0.2810\n",
            "Epoch 29/200\n",
            "126433/126433 [==============================] - 117s 922us/step - loss: 3.5359 - accuracy: 0.2836\n",
            "Epoch 30/200\n",
            "126433/126433 [==============================] - 116s 916us/step - loss: 3.5074 - accuracy: 0.2874\n",
            "Epoch 31/200\n",
            "126433/126433 [==============================] - 115s 913us/step - loss: 3.4794 - accuracy: 0.2902\n",
            "Epoch 32/200\n",
            "126433/126433 [==============================] - 118s 932us/step - loss: 3.4494 - accuracy: 0.2951\n",
            "Epoch 33/200\n",
            "126433/126433 [==============================] - 117s 926us/step - loss: 3.4219 - accuracy: 0.2977\n",
            "Epoch 34/200\n",
            "126433/126433 [==============================] - 117s 923us/step - loss: 3.3964 - accuracy: 0.2999\n",
            "Epoch 35/200\n",
            "126433/126433 [==============================] - 117s 923us/step - loss: 3.3698 - accuracy: 0.3046\n",
            "Epoch 36/200\n",
            "126433/126433 [==============================] - 117s 925us/step - loss: 3.3406 - accuracy: 0.3077\n",
            "Epoch 37/200\n",
            "126433/126433 [==============================] - 117s 922us/step - loss: 3.3160 - accuracy: 0.3111\n",
            "Epoch 38/200\n",
            "126433/126433 [==============================] - 116s 921us/step - loss: 3.2925 - accuracy: 0.3148\n",
            "Epoch 39/200\n",
            "126433/126433 [==============================] - 116s 919us/step - loss: 3.2655 - accuracy: 0.3192\n",
            "Epoch 40/200\n",
            "126433/126433 [==============================] - 116s 919us/step - loss: 3.2412 - accuracy: 0.3210\n",
            "Epoch 41/200\n",
            "126433/126433 [==============================] - 117s 927us/step - loss: 3.2169 - accuracy: 0.3246\n",
            "Epoch 42/200\n",
            "126433/126433 [==============================] - 117s 924us/step - loss: 3.1932 - accuracy: 0.3282\n",
            "Epoch 43/200\n",
            "126433/126433 [==============================] - 116s 918us/step - loss: 3.1693 - accuracy: 0.3321\n",
            "Epoch 44/200\n",
            "126433/126433 [==============================] - 117s 925us/step - loss: 3.1461 - accuracy: 0.3345\n",
            "Epoch 45/200\n",
            "126433/126433 [==============================] - 116s 919us/step - loss: 3.1223 - accuracy: 0.3379\n",
            "Epoch 46/200\n",
            "126433/126433 [==============================] - 117s 927us/step - loss: 3.0988 - accuracy: 0.3409\n",
            "Epoch 47/200\n",
            "126433/126433 [==============================] - 118s 936us/step - loss: 3.0763 - accuracy: 0.3442\n",
            "Epoch 48/200\n",
            "126433/126433 [==============================] - 118s 934us/step - loss: 3.0528 - accuracy: 0.3489\n",
            "Epoch 49/200\n",
            "126433/126433 [==============================] - 117s 929us/step - loss: 3.0266 - accuracy: 0.3519\n",
            "Epoch 50/200\n",
            "126433/126433 [==============================] - 117s 929us/step - loss: 3.0041 - accuracy: 0.3559\n",
            "Epoch 51/200\n",
            "126433/126433 [==============================] - 117s 925us/step - loss: 2.9838 - accuracy: 0.3594\n",
            "Epoch 52/200\n",
            "126433/126433 [==============================] - 117s 926us/step - loss: 2.9602 - accuracy: 0.3615\n",
            "Epoch 53/200\n",
            "126433/126433 [==============================] - 116s 918us/step - loss: 2.9366 - accuracy: 0.3663\n",
            "Epoch 54/200\n",
            "126433/126433 [==============================] - 116s 921us/step - loss: 2.9135 - accuracy: 0.3693\n",
            "Epoch 55/200\n",
            "126433/126433 [==============================] - 117s 928us/step - loss: 2.8943 - accuracy: 0.3727\n",
            "Epoch 56/200\n",
            "126433/126433 [==============================] - 118s 930us/step - loss: 2.8696 - accuracy: 0.3774\n",
            "Epoch 57/200\n",
            "126433/126433 [==============================] - 117s 929us/step - loss: 2.8468 - accuracy: 0.3810\n",
            "Epoch 58/200\n",
            "116480/126433 [==========================>...] - ETA: 9s - loss: 2.8179 - accuracy: 0.3858Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJO3ZbTNiygV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "8757e47b-8077-42c9-bab0-fcdbd320d548"
      },
      "source": [
        "# load the model\n",
        "model = load_model('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pWQDnC1bVYy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "bfa11354-61da-4c74-f394-f10384d8be24"
      },
      "source": [
        "seed_sentence = \"ssttaarrt he is a very poor student eenndd ssttaarrt he is a greedy friend eenndd ssttaarrt he get late to\"\n",
        "seed_sentence_enumerated = []\n",
        "for item in seed_sentence.split(\" \"):\n",
        "  temp = word_enum_dict[item]\n",
        "  seed_sentence_enumerated.append(temp)\n",
        "\n",
        "seed_sentence_enumerated = np.array([seed_sentence_enumerated])\n",
        "print (seed_sentence_enumerated, seed_sentence_enumerated.shape)\n",
        "\n",
        "\n",
        "#seed_sentence_enumerated_array = np.array([seed_sentence_enumerated])\n",
        "#seed_sentence_enumerated_array\n",
        "#seed_sentence_enumerated_array.ndim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[7297   90 1981 1813 3180 2688 5608 7298 7297   90 1981 1813 3868 2884\n",
            "  7298 7297   90  962 5925 1553]] (1, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWQxL72Zhadz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "ff89f70d-bf4b-4c74-f7a3-832d627d09da"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate'] \n",
            "\n",
            " [[4882 1553  551 6463 2964  551 1123 2055  109 1270 1094 5123  212 1553\n",
            "  5811 3161 5876 2815 6543  333]] (1, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhfsBXR5b6Sd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "22539aea-bcdf-41dd-f41e-080d836ebd2c"
      },
      "source": [
        "i = 21\n",
        "seed_sentence = [item for item in \" \".join(unified_corpora).split(\" \")[i:i+20]]\n",
        "seed_sentence_enumerated = np.array([[word_enum_dict[item] for item in seed_sentence[0:20]]])\n",
        "predicted_sentence =\"\"\n",
        "for i in range(0, 100):\n",
        "  output = model.predict_classes(seed_sentence_enumerated)\n",
        "  seed_sentence_enumerated = np.array([seed_sentence_enumerated.tolist()[0][1:] + [output[0]]])\n",
        "  predicted_word = reversed_word_enum_dict[output[0]]\n",
        "  if predicted_word == 'ssttaarrt':\n",
        "    predicted_word = \" \"\n",
        "  if predicted_word == 'eenndd':\n",
        "    predicted_word = \".\"\n",
        "\n",
        "  predicted_sentence = predicted_sentence+\" \"+predicted_word\n",
        "\n",
        "print (\"Seed\\n\", \" \".join(seed_sentence))\n",
        "print (\"\\n\\n\\nGenerated text:\\n\", predicted_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed\n",
            " prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would\n",
            "\n",
            "\n",
            "\n",
            "Generated text:\n",
            "  occur diseased artificers observes distinctness reply therefore dicast fawns families fawns bearing imaginary independently niceratus lessons practitioner valiantly wins waging poor independently referring lot unreasoning quite .   wins independently expenditure practitioner independently recognise .   wins shield unreasoning reappeared independently origin practitioner independently books distinctness independently arrives .   wins watering bearing independently never distinctness prosecutions fawns particulars wins artificers bearing fawns particulars string mainly diviner fawns previous laying calls fawns particulars independently cleverest distinctness independently softness seasoning independently joint distinctness yelping independently expeditions .   wins temper boxing practitioner independently regularity distinctness tolls infinitely seductions wins apply .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7K2GbRMl4eU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "d36a6f39-b423-458c-e51f-df49b01f4d8a"
      },
      "source": [
        "seed_sentence = [item for item in \" \".join(unified_corpora).split(\" \")[i:i+120]]\n",
        "seed_sentence_enumerated = np.array([[word_enum_dict[item] for item in seed_sentence[0:20]]])\n",
        "print(len(seed_sentence_enumerated[0]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n",
            "[[10501 10501 10232  9063 10391  6931  5929 10484 10905 10779  5929  8072\n",
            "  11827  9231  6982 10501  9901  7327 12313  7189]] (1, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd4hfC1Im7cC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "939f173f-8e2f-4cd8-e17b-70b680e3acc3"
      },
      "source": [
        "temp = np.array([seed_sentence_enumerated.tolist()[0][1:] + [output[0]]])\n",
        "#temp1 = seed_sentence_enumerated.tolist()[0] + output\n",
        "print(seed_sentence_enumerated.tolist()[0],\"\\n\",output, \"\\n\", temp, temp.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5123, 5123, 4854, 3685, 5013, 1553, 551, 5106, 5527, 5401, 551, 2694, 6449, 3853, 1604, 5123, 4523, 1949, 6935, 1811] \n",
            " [5378] \n",
            " [[5123 4854 3685 5013 1553  551 5106 5527 5401  551 2694 6449 3853 1604\n",
            "  5123 4523 1949 6935 1811 5378]] (1, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ofqrYZCI-5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the network weights\n",
        "filename = \"weights-improvement-20-2.0532.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAnOOnMCciT8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "1b762d05-b83a-4c45-8444-ded848feb587"
      },
      "source": [
        "# pick a random seed\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print (\"Seed:\")\n",
        "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "print(\"\\n\\n Generating Chars:\")\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = np.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print (\"\\nDone.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-9cdee3ae940e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# pick a random seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Seed:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint_to_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    }
  ]
}
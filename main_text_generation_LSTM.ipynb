{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_text_generation_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP1o0znXgOXqL98LQ1L2F96",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhijitsahoo0790/text_generation_using_LSTM/blob/master/main_text_generation_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ugM54h3E41H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "5b23a176-e7db-46ed-a784-4ff735752dc6"
      },
      "source": [
        "#Mount google drive to google Colab environment\n",
        "from os.path import join\n",
        "from google.colab import drive\n",
        "\n",
        "ROOT = \"/content/drive\"\n",
        "drive.mount(ROOT)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb6LXyNYFJN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "For creating a new project in GitHub, it will throw error if it is executed after the project dir is created\n",
        "\"\"\"\n",
        "ROOT = \"/content/drive\"\n",
        "PROJ = \"My Drive/Colab Notebooks/text_generation_using_LSTM\" # This is a custom path.\n",
        "PROJECT_PATH = join(ROOT, PROJ)\n",
        "!mkdir \"{PROJECT_PATH}\"\n",
        "!git clone https://github.com/abhijitsahoo0790/text_generation_using_LSTM.git \"{PROJECT_PATH}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r28Ok3GGoJKd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "55430308-a909-4fd5-afa0-3ade618ab1bc"
      },
      "source": [
        "ROOT = \"/content/drive\"\n",
        "PROJ = \"My Drive/Colab Notebooks/text_generation_using_LSTM\" # This is a custom path.\n",
        "PROJECT_PATH = join(ROOT, PROJ)\n",
        "%cd \"{PROJECT_PATH}\"\n",
        "!git pull origin master"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/text_generation_using_LSTM\n",
            "remote: Enumerating objects: 35, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 27 (delta 14), reused 24 (delta 11), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (27/27), done.\n",
            "From https://github.com/abhijitsahoo0790/text_generation_using_LSTM\n",
            " * branch            master     -> FETCH_HEAD\n",
            "   6b9a15e..ff3aeba  master     -> origin/master\n",
            "Updating 6b9a15e..ff3aeba\n",
            "error: The following untracked working tree files would be overwritten by merge:\n",
            "\tdata/republic.txt\n",
            "Please move or remove them before you merge.\n",
            "Aborting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N40o2oB7GVw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "import re\n",
        "import copy\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import traceback\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s %(levelname)-8s %(message)s', \n",
        "                    filename='log.txt', filemode='w', level=logging.DEBUG, \n",
        "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
        "START_DELIMITER = \"ssttaarrt\"\n",
        "END_DELIMITER = \"eenndd\"\n",
        "WINDOW_LENGTH = 20"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LqDFQH6r2zq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fetch_the_corpora_using_NLTK():\n",
        "    \"\"\"\n",
        "    Return the unified corpora from NLTK corpora.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    text : str\n",
        "        Text data of the corpora.\n",
        "    \"\"\"\n",
        "    corpous_name = \"brown\"\n",
        "    status = nltk.download(corpous_name)\n",
        "    if (status):\n",
        "        logging.info(\"Downloaded Brown corpus\")\n",
        "        mdetok = TreebankWordDetokenizer()\n",
        "        brown_natural = [mdetok.detokenize(' '.join(sent).replace('``', '\"').replace(\"''\", '\"').replace('`', \"'\").split())  for sent in brown.sents()]\n",
        "        logging.info(\"Processed Brown corpus as text\")\n",
        "    else:\n",
        "        logging.error(\"Couldn't download the \"+ corpous_name+\" corpus\")\n",
        "        \n",
        "    return brown_natural\n",
        "\n",
        "def enumerate_text_using_word_enum_dict(unified_corpora, word_enum_dict):\n",
        "    \"\"\"\n",
        "    Enumerate the complete text in corpous using word_enum_dict\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    unified_corpora : TYPE\n",
        "        DESCRIPTION.\n",
        "    word_enum_dict : TYPE\n",
        "        DESCRIPTION.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    complete_text_enumerated : TYPE\n",
        "        DESCRIPTION.\n",
        "\n",
        "    \"\"\"\n",
        "    complete_text = \" \".join([START_DELIMITER+\" \"+item+\" \"+END_DELIMITER for item in unified_corpora])\n",
        "    complete_text_processed = re.sub(' +', ' ', re.sub('[^A-Za-z ]+', ' ',complete_text.lower())).strip()\n",
        "    complete_text_enumerated =  [word_enum_dict[item] for item in complete_text_processed.split(\" \") if item in word_enum_dict]   \n",
        "    return complete_text_enumerated\n",
        "\n",
        "def enumerate_unique_words(text_corpus):\n",
        "    \"\"\"\n",
        "    Enumerate unique words and return its dictionary and reversed-dictionary\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    unified_corpora : list of str\n",
        "        The text corpora as a list of words \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    word_enum_dict\n",
        "        word as key and its integer enumeration as the value.\n",
        "    reversed_word_enum_dict\n",
        "        word as value and its integer enumeration as the key.\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    Join all sentences, remove special characters except Space, split all \n",
        "    words, take set for unique words, convert it to list, remove None values using filter\n",
        "    \"\"\"\n",
        "    unique_words = list(filter(None, list(set(re.sub('[^A-Za-z ]+', ' ', (text_corpus)).split(\" \")))))\n",
        "    unique_words = unique_words + [START_DELIMITER, END_DELIMITER]\n",
        "    #enumerate unique words\n",
        "    word_enum_dict = {v:k for k,v in enumerate(unique_words)}\n",
        "    reversed_word_enum_dict = {k:v for k,v in enumerate(unique_words)}\n",
        "    return [word_enum_dict, reversed_word_enum_dict]\n",
        "\n",
        "def generate_sequence_data_for_LSTM(complete_text_enumerated):\n",
        "    \"\"\"\n",
        "    Generate pattern sequences of length as specified by WINDOW_LENGTH and \n",
        "    also generate target of the patterns generated.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    complete_text_enumerated : list of int\n",
        "        Enumerated text sequence.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X\n",
        "        Reshaped pattern sequences for LSTM input .\n",
        "    y\n",
        "        Target for each generated patterns.\n",
        "    \"\"\"\n",
        "    pattern_sequence = []\n",
        "    pattern_targets = []\n",
        "    for i in range(0, len(complete_text_enumerated)-WINDOW_LENGTH):\n",
        "        temp_pattern = complete_text_enumerated[i:i+WINDOW_LENGTH]\n",
        "        temp_pattern_target = complete_text_enumerated[i+WINDOW_LENGTH]\n",
        "        pattern_sequence.append(temp_pattern)\n",
        "        pattern_targets.append(temp_pattern_target)\n",
        "    num_patterns = len(pattern_sequence)\n",
        "    X = np.reshape(pattern_sequence, (num_patterns, WINDOW_LENGTH))\n",
        "    y = np_utils.to_categorical(pattern_targets)\n",
        "    return [X, y]\n",
        "\n",
        "\n",
        "def remove_special_chars(text):\n",
        "    \"\"\"\n",
        "    Remove all special characters except space and remove extra spaces. \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        Any text\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    text : str\n",
        "        Processed text\n",
        "    \"\"\"\n",
        "    \n",
        "    text = re.sub(' +', ' ', re.sub('[^A-Za-z ]+', ' ', text).strip())\n",
        "    return text\n",
        "\n",
        "def fetch_corpous_from_file(filepath):    \n",
        "    \"\"\"\n",
        "    Read a corpus and do basic processing.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filepath : str\n",
        "        Path of the text corpus\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    corpus_list_sent_processed : list of str\n",
        "        processed corpus in form of list of str.\n",
        "\n",
        "    \"\"\"\n",
        "    f = open(filepath, 'r')\n",
        "    corpus_text = f.read()    \n",
        "    corpus_sentence_list = corpus_text.lower().split('.')\n",
        "    corpus_list_sent_processed = [remove_special_chars(item) for item in corpus_sentence_list if len(item)>1] \n",
        "    return corpus_list_sent_processed"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgoQIOBgGa6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4a2ccb61-19da-46d2-8212-2d68934d5b26"
      },
      "source": [
        "PROJ_PATH = \"/content/drive/My Drive/Colab Notebooks/text_generation_using_LSTM/\"\n",
        "FILE_PATH = \"data/republic.txt\"\n",
        "FULL_PATH_FILE = join(PROJ_PATH, FILE_PATH)\n",
        "%cd \"{PROJ_PATH}\"\n",
        "if __name__ == \"__main__\":\n",
        "    logging.info(\"Fetching text corpus...\")\n",
        "    # unified_corpora = fetch_the_corpora_using_NLTK() \n",
        "    unified_corpora = fetch_corpous_from_file(FULL_PATH_FILE)  \n",
        "    logging.info(\"Fetched text corpus\")\n",
        "    \n",
        "    # Enumerate unique words\n",
        "    [word_enum_dict, reversed_word_enum_dict] = enumerate_unique_words(\" \".join(unified_corpora))\n",
        "    # Enumerate text using word_enum_dict\n",
        "    complete_text_enumerated = enumerate_text_using_word_enum_dict(unified_corpora, word_enum_dict)\n",
        "    # generate sequence data for training LSTM\n",
        "    [X, y] = generate_sequence_data_for_LSTM(complete_text_enumerated)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/text_generation_using_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLDv2rzrLLjk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "outputId": "c76c286c-c30f-4d41-f88a-21c4bd5dbb20"
      },
      "source": [
        "vocab_size = len(word_enum_dict) + 1\n",
        "seq_length = X.shape[1]\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "model.add(LSTM(100, input_shape=(X.shape[0], X.shape[1]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size-1, activation='softmax'))\n",
        "print(model.summary())    \n",
        "print (X.shape, y.shape)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# define the checkpoint\n",
        "filepath=\"results/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "# fit the model\n",
        "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (None, 20, 50)            365000    \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (None, 20, 100)           60400     \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 20, 100)           0         \n",
            "_________________________________________________________________\n",
            "lstm_11 (LSTM)               (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 7299)              737199    \n",
            "=================================================================\n",
            "Total params: 1,253,099\n",
            "Trainable params: 1,253,099\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "(126433, 20) (126433, 7299)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            " 85888/126433 [===================>..........] - ETA: 49s - loss: 6.0962"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz3mzR-syhid",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eed8a551-5791-43cd-9458-2f2483d642f0"
      },
      "source": [
        "vocab_size = len(word_enum_dict) + 1\n",
        "vocab_size"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ofqrYZCI-5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the network weights\n",
        "filename = \"weights-improvement-20-2.0532.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAnOOnMCciT8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "1b762d05-b83a-4c45-8444-ded848feb587"
      },
      "source": [
        "# pick a random seed\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print (\"Seed:\")\n",
        "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "print(\"\\n\\n Generating Chars:\")\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = np.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print (\"\\nDone.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-9cdee3ae940e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# pick a random seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Seed:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint_to_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    }
  ]
}
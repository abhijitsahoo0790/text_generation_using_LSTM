{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_text_generation_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOu+Ltm5VgDujDC79JosIwc"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ugM54h3E41H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "30d87824-5f99-4930-ab9d-31b4a2914d02"
      },
      "source": [
        "#Mount google drive to google Colab environment\n",
        "from os.path import join\n",
        "from google.colab import drive\n",
        "\n",
        "ROOT = \"/content/drive\"\n",
        "drive.mount(ROOT)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb6LXyNYFJN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "For creating a new project in GitHub, it will throw error if it is executed after the project dir is created\n",
        "\"\"\"\n",
        "ROOT = \"/content/drive\"\n",
        "PROJ = \"My Drive/Colab Notebooks/text_generation_using_LSTM\" # This is a custom path.\n",
        "PROJECT_PATH = join(ROOT, PROJ)\n",
        "!mkdir \"{PROJECT_PATH}\"\n",
        "!git clone https://github.com/abhijitsahoo0790/text_generation_using_LSTM.git \"{PROJECT_PATH}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r28Ok3GGoJKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ROOT = \"/content/drive\"\n",
        "PROJ = \"My Drive/Colab Notebooks/text_generation_using_LSTM\" # This is a custom path.\n",
        "PROJECT_PATH = join(ROOT, PROJ)\n",
        "%cd \"{PROJECT_PATH}\"\n",
        "!git pull origin master"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N40o2oB7GVw2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25f3f19b-8fb5-43ac-a940-33cbdb9bdb39"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from random import randint\n",
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "import copy\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import traceback\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s %(levelname)-8s %(message)s', \n",
        "                    filename='log.txt', filemode='w', level=logging.DEBUG, \n",
        "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
        "START_DELIMITER = \"ssttaarrt\" #This is starting delimiter for a sentence\n",
        "END_DELIMITER = \"eenndd\" #This is starting delimiter for a sentence\n",
        "WINDOW_LENGTH = 100"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LqDFQH6r2zq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fetch_the_corpora_using_NLTK():\n",
        "    \"\"\"\n",
        "    Return the unified corpora from NLTK corpora.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    text : str\n",
        "        Text data of the corpora.\n",
        "    \"\"\"\n",
        "    corpous_name = \"brown\"\n",
        "    status = nltk.download(corpous_name)\n",
        "    if (status):\n",
        "        logging.info(\"Downloaded Brown corpus\")\n",
        "        mdetok = TreebankWordDetokenizer()\n",
        "        brown_natural = [mdetok.detokenize(' '.join(sent).replace('``', '\"').replace(\"''\", '\"').replace('`', \"'\").split())  for sent in brown.sents()]\n",
        "        logging.info(\"Processed Brown corpus as text\")\n",
        "    else:\n",
        "        logging.error(\"Couldn't download the \"+ corpous_name+\" corpus\")\n",
        "        \n",
        "    return brown_natural\n",
        "\n",
        "def enumerate_text_using_word_enum_dict(unified_corpora, word_enum_dict):\n",
        "    \"\"\"\n",
        "    Enumerate the complete text in corpous using word_enum_dict\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    unified_corpora : TYPE\n",
        "        DESCRIPTION.\n",
        "    word_enum_dict : TYPE\n",
        "        DESCRIPTION.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    complete_text_enumerated : TYPE\n",
        "        DESCRIPTION.\n",
        "\n",
        "    \"\"\"\n",
        "    complete_text = \" \".join([START_DELIMITER+\" \"+item+\" \"+END_DELIMITER for item in unified_corpora])\n",
        "    complete_text_processed = re.sub(' +', ' ', re.sub('[^A-Za-z ]+', ' ',complete_text.lower())).strip()\n",
        "    complete_text_enumerated =  [word_enum_dict[item] for item in complete_text_processed.split(\" \") if item in word_enum_dict]   \n",
        "    return complete_text_enumerated\n",
        "\n",
        "def enumerate_unique_words(text_corpus):\n",
        "    \"\"\"\n",
        "    Enumerate unique words and return its dictionary and reversed-dictionary\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    unified_corpora : list of str\n",
        "        The text corpora as a list of words \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    word_enum_dict\n",
        "        word as key and its integer enumeration as the value.\n",
        "    reversed_word_enum_dict\n",
        "        word as value and its integer enumeration as the key.\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    Join all sentences, remove special characters except Space, split all \n",
        "    words, take set for unique words, convert it to list, remove None values using filter\n",
        "    \"\"\"\n",
        "    unique_words = list(filter(None, list(set(re.sub('[^A-Za-z ]+', ' ', (text_corpus)).split(\" \")))))\n",
        "    unique_words = unique_words + [START_DELIMITER, END_DELIMITER]\n",
        "    #enumerate unique words\n",
        "    word_enum_dict = {v:k for k,v in enumerate(unique_words)}\n",
        "    reversed_word_enum_dict = {k:v for k,v in enumerate(unique_words)}\n",
        "    return [word_enum_dict, reversed_word_enum_dict]\n",
        "\n",
        "def generate_sequence_data_for_LSTM(complete_text_enumerated):\n",
        "    \"\"\"\n",
        "    Generate pattern sequences of length as specified by WINDOW_LENGTH and \n",
        "    also generate target of the patterns generated.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    complete_text_enumerated : list of int\n",
        "        Enumerated text sequence.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X\n",
        "        Reshaped pattern sequences for LSTM input .\n",
        "    y\n",
        "        Target for each generated patterns.\n",
        "    \"\"\"\n",
        "    pattern_sequence = []\n",
        "    pattern_targets = []\n",
        "    for i in range(0, len(complete_text_enumerated)-WINDOW_LENGTH):\n",
        "        temp_pattern = complete_text_enumerated[i:i+WINDOW_LENGTH]\n",
        "        temp_pattern_target = complete_text_enumerated[i+WINDOW_LENGTH]\n",
        "        pattern_sequence.append(temp_pattern)\n",
        "        pattern_targets.append(temp_pattern_target)\n",
        "    num_patterns = len(pattern_sequence)\n",
        "    X = np.reshape(pattern_sequence, (num_patterns, WINDOW_LENGTH))\n",
        "    y = np_utils.to_categorical(pattern_targets)\n",
        "    return [X, y]\n",
        "\n",
        "\n",
        "def remove_special_chars(text):\n",
        "    \"\"\"\n",
        "    Remove all special characters except space and remove extra spaces. \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        Any text\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    text : str\n",
        "        Processed text\n",
        "    \"\"\"\n",
        "    \n",
        "    text = re.sub(' +', ' ', re.sub('[^A-Za-z ]+', ' ', text).strip())\n",
        "    return text\n",
        "\n",
        "def fetch_corpous_from_file(filepath):    \n",
        "    \"\"\"\n",
        "    Read a corpus and do basic processing.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filepath : str\n",
        "        Path of the text corpus\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    corpus_list_sent_processed : list of str\n",
        "        processed corpus in form of list of str.\n",
        "\n",
        "    \"\"\"\n",
        "    f = open(filepath, 'r')\n",
        "    corpus_text = f.read()    \n",
        "    corpus_sentence_list = corpus_text.lower().split('.')\n",
        "    corpus_list_sent_processed = [remove_special_chars(item) for item in corpus_sentence_list if len(item)>1] \n",
        "    return corpus_list_sent_processed"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgoQIOBgGa6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2aacaedc-3524-44fa-8610-2c526a41bd2f"
      },
      "source": [
        "PROJ_PATH = \"/content/drive/My Drive/Colab Notebooks/text_generation_using_LSTM/\"\n",
        "FILE_PATH = \"data/republic.txt\"\n",
        "FULL_PATH_FILE = join(PROJ_PATH, FILE_PATH)\n",
        "%cd \"{PROJ_PATH}\"\n",
        "if __name__ == \"__main__\":\n",
        "    logging.info(\"Fetching text corpus...\")\n",
        "    # unified_corpora = fetch_the_corpora_using_NLTK() \n",
        "    unified_corpora = fetch_corpous_from_file(FULL_PATH_FILE)  \n",
        "    logging.info(\"Fetched text corpus\")\n",
        "    \n",
        "    # Enumerate unique words\n",
        "    [word_enum_dict, reversed_word_enum_dict] = enumerate_unique_words(\" \".join(unified_corpora))\n",
        "    # Enumerate text using word_enum_dict\n",
        "    complete_text_enumerated = enumerate_text_using_word_enum_dict(unified_corpora, word_enum_dict)\n",
        "    # generate sequence data for training LSTM\n",
        "    [X, y] = generate_sequence_data_for_LSTM(complete_text_enumerated)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/text_generation_using_LSTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLDv2rzrLLjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Train a sequence model to generate text\n",
        "\"\"\"\n",
        "vocab_size = len(word_enum_dict) + 1\n",
        "seq_length = WINDOW_LENGTH = 100\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size-1, activation='softmax'))\n",
        "print(model.summary())\n",
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "model.fit(X, y, batch_size=128, epochs=200)\n",
        "# save the model to file\n",
        "#model.save('model_batchsize128_window100.h5') #mention the file name with a mention of parameters to refer and load\n",
        "\n",
        "# define the checkpoint\n",
        "#filepath=\"results/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "#checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "#callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA4MdEyKRSKb",
        "colab_type": "text"
      },
      "source": [
        "**Now after training the model, I have saved it. Now I will be loading it below for generating text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJO3ZbTNiygV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the model\n",
        "model = load_model('model_batchsize128.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il7-9X06RciF",
        "colab_type": "text"
      },
      "source": [
        "####Now, the task is to generate the next n words of text(say n=100), given a seed sentence.\n",
        "\n",
        "####The output could have been better with sequence length of 100 as the above codes suggest with variable WINDOW_LENGTH, but due to limited GPU access from google colab I trained it upto 20 length sequence. Feel free to play with the following parameters for better results,\n",
        "\n",
        "####1) Sequence length (WINDOW_LENGTH) : int, more the value better would be the results but after a point it might overfit. Length up to 200 should be fine in  my opinion. Just change the length of the seed sentence to the length of WINDOW_LENGTH. Also, the model is trained over only around 7250 vocabs for computational reasons, so, a lot of words would be missing and would generate key error for custom seeds. You need to replace those new words to resolve the issue.\n",
        "\n",
        "####2) epoch = more the number of epoch more the accuracy and lesser the error\n",
        "\n",
        "####3) batch size = Ideal would be 128 for this case, but one can increase it for faster process. But increasing this would degrade the result quality\n",
        "\n",
        "####That's all. Enjoy the \"Text Generation Task\".*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhfsBXR5b6Sd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "cac97a5d-e1a5-4e16-89d1-71ca780fef12"
      },
      "source": [
        "\"\"\"\n",
        "Task 1:\n",
        "Choose a random seed sentence (or from any specified word sequence as specified by i) and generate next 100 words\n",
        "\"\"\"\n",
        "corpus_word_seq_list = \" \".join(unified_corpora).split(\" \")\n",
        "i = 10\n",
        "#i = randint(0, len(corpus_word_seq_list))\n",
        "\n",
        "seed_sentence = [item for item in corpus_word_seq_list[i:i+20]]\n",
        "seed_sentence_enumerated = np.array([[word_enum_dict[item] for item in seed_sentence[0:20]]])\n",
        "predicted_sentence =\"\"\n",
        "for i in range(0, 100):\n",
        "  output = model.predict_classes(seed_sentence_enumerated)\n",
        "  seed_sentence_enumerated = np.array([seed_sentence_enumerated.tolist()[0][1:] + [output[0]]])\n",
        "  predicted_word = reversed_word_enum_dict[output[0]]\n",
        "  if predicted_word == 'ssttaarrt':\n",
        "    predicted_word = \" \"\n",
        "  if predicted_word == 'eenndd':\n",
        "    predicted_word = \".\"\n",
        "\n",
        "  predicted_sentence = predicted_sentence+\" \"+predicted_word\n",
        "\n",
        "print (\"Seed\\n\", \" \".join(seed_sentence))\n",
        "print (\"\\n\\n\\nGenerated text:\\n\",\"'\", \" \".join(seed_sentence),\"' \", predicted_sentence)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed\n",
            " glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and\n",
            "\n",
            "\n",
            "\n",
            "Generated text:\n",
            " ' glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and '   unconsciously sharers weakly victorious ascertained victorious especially vacant .   imply excellence have girt hades epidemic reluctantly represents demonstrated excellence recollection unconvinced allurements .   vine numbering excited unconvinced lydian imply necessary enter hades methinks victorious fashioning atalanta victorious silenced three alarms methinks demonstrated victorious friends enter wretchedest .   enter fraud obeying victorious ally three light demonstrated horseback sharers lets assailing wasted mostly inconsistent descendants artemis descendants cups celebrating methinks especially there methinks citizen singers rebuking light perceives reascend wet pursue unconvinced lydian outlines reported enter aught choose sinners sharers mistake lived carpentering demonstrated diseased lofty ken various methinks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWlxiDx_PNUE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "07ec4100-8ad5-4b6e-ed7a-0af2c24b0378"
      },
      "source": [
        "\"\"\"\n",
        "Task 2:\n",
        "Provide your own custom seed sentence that may not be there in the corpus \n",
        "\"\"\"\n",
        "seed_sentence = \"ssttaarrt that is especially has some of the world largest and most renowned victory including the last one which is\"\n",
        "seed_sentence_enumerated = []\n",
        "for item in seed_sentence.split(\" \"):\n",
        "  temp = word_enum_dict[item]\n",
        "  seed_sentence_enumerated.append(temp)\n",
        "seed_sentence_enumerated = np.array([seed_sentence_enumerated])\n",
        "\n",
        "predicted_sentence =\"\"\n",
        "for i in range(0, 100):\n",
        "  output = model.predict_classes(seed_sentence_enumerated)\n",
        "  seed_sentence_enumerated = np.array([seed_sentence_enumerated.tolist()[0][1:] + [output[0]]])\n",
        "  predicted_word = reversed_word_enum_dict[output[0]]\n",
        "  if predicted_word == 'ssttaarrt':\n",
        "    predicted_word = \" \"\n",
        "  if predicted_word == 'eenndd':\n",
        "    predicted_word = \".\"\n",
        "\n",
        "  predicted_sentence = predicted_sentence+\" \"+predicted_word\n",
        "\n",
        "print (\"Seed\\n\", seed_sentence)\n",
        "print (\"\\n\\n\\nGenerated text:\\n\",\"'\", seed_sentence,\"' \", predicted_sentence)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed\n",
            " ssttaarrt that is especially has some of the world largest and most renowned victory including the last one which is\n",
            "\n",
            "\n",
            "\n",
            "Generated text:\n",
            " ' ssttaarrt that is especially has some of the world largest and most renowned victory including the last one which is '   sharers kingdom exercise reascend companion sharers victorious purpose enter graves believe rebuking bulk fit enter conditions adherents enter sharers disciple nice enter sharers victorious satellites .   assailing wet comprehended lets assailing spontaneously .   fraud aught countless hirelings numbering shades sharers enact imply demanding colleagues allotted embroidery sharers acts discourse colleagues ranks heracleitus recollection fare defects downwards garland sharers dull affinities victorious undoubtedly goal enter vigorously diseased acknowledged methinks reascend realization numbering claim enter numbering have pottery full various atalanta pleasanter propose commit heracleitus recollection housekeeping flinch pleasanter methinks demonstrated victorious furnished begged lets three assailing notice harmonies enter\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}